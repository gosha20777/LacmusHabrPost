# Проект Lacmus: как компьютерное зрение помогает спасать заблудившихся людей  
![](img/logo.png)  

### Введение  

Всем привет!  

Возможно, вы уже слышали про инициативу Machine Learning for Social Good (**#ml4sg**) сообщества [Open Data Science](https://ods.ai/). 
В её рамках энтузиасты на бесплатной основе применяют методы машинного обучения для решения социально-значимых проблем. 
Мы, команда проекта Lacmus, занимаемся внедрением современных Deep Learning-решений для поиска людей, потерявшихся вне населённой местности: в лесу, поле и т.д..

![](img/kdpv.png)

По приблизительным оценкам, в России каждый год пропадает более ста тысяч человек. 
Ощутимую часть из них составляют люди, заблудившиеся вдали от человеческого жилья. 
Некоторые из потерявшихся, к счастью, выбираются сами, для помощи другим мобилизуются добровольческие поисково-спасательные отряды (ПСО). 
Наиболее известным ПСО является, пожалуй, Liza Alert, но хочется отметить, что он отнюдь не единственный.

Основными способами поиска на данный момент, в XXI-ом веке, является пешее прочёсывание окрестностей с применением технических средств, которые зачастую не сложнее сирены или гудящего маяка. Тема, конечно, актуальная и горячая, порождает много идей по использованию в поисках достижений научно-технического прогресса; некоторые из них даже воплощаются в виде прототипов и тестируются в специально организованных конкурсах. Но лес - это лес, и реальные условия поисков вкупе с ограниченностью материальных ресурсов делают эту проблему сложной и ещё очень далёкой от полного решения.  
Для осмотра больших участков территории в последнее время спасатели все чаще применяют беспилотные летательные аппараты (БПЛА), фотографирующие местность с высоты 40-50м. С одной поисково-спасательной операции получается несколько тысяч фотографий, которые на сегодняшний день добровольцы отсматривают вручную. Понятно, что такая обработка это долго и неэффективно, через два часа такой работы люди устают и не могут продолжать поиск - а ведь от его скорости зависит здоровье и жизнь людей.  

Совместно с поисково-спасательными отрядами мы занимаемся разработкой программы для поиска пропавших людей на снимках, сделанных с БПЛА. Как специалисты по машинному обучению, мы стараемся сделать поиск автоматическим и быстрым.


### Аналогичные решения для БПЛА    

Несправедливо было бы утверждать, что Lacmus - единственный проект, разрабатываемый в данном направлении. Однако складывется впечатление, что он - единственный, разрабатываемый в тесном взаимодействии с ПСО, и ориентированный на их насущные потребности и возможности. Некоторое время назад был проведен конкурс "Одиссея", в котором разные команды представили свои решения для поиска и спасения людей, в том числе с помощью БПЛА. Находясь на начальной стадии разработки, мы присутствовали на этом конкурсе исключительно в качестве наблюдателей. Но сопоставив результаты конкурса, сведения о подобных проектах и наш опыт общения с такими ПСО как Liza Alert и Сова, хочется отметить проблемы, свойственные многим аналогам:  

- Стоимость внедрения. Некоторые команды из конкурса "Одиссея" занялись разработкой инноваций: собственных дронов и БПЛА. Но надо понимать, что ПСО в России обычно работают на некоммерческой основе, и оснащать операторов дронов машинами стоимостью  от 1 000 000 рублей - это слишком дорого. К тому же, мало просто произвести летательный аппарат, нужно наладить его обслуживание. Мелким компаниям сложно предложить решения за те же деньги, что и у суровых китайских конкурентов.

- Коммерческая направленность многих решений. Нет ничего плохого в бизнес-проектах (которые кормят большую часть команды на основной работе). Но поиск потерявшихся в лесу лююей - довольно специфичная задача, не всякий коммерческий проект можно в нее встроить. Можно разработать первоклассный уникальный дрон и воткнуть туда нейроночку, распознающую всё, что можно распознать в природных условиях, но для поиска людей в лесу силами добровольческий ПСО такой проект вряд ли пригодится: тут нужно максемально дешевое, но эффективное решение. В конце концов, БПЛА может сам упасть и навсегда потеряться в глубине леса. Дорогие мультиканальные камеры тут не подойдут, тоько RGB, только хардкор. Отпадают и дорогие тепловизоры - а дешевые модели имеют слишком низкое разрешение. (И в целом, тепловизоры тут не эффективны, потому что замерзший в лесу человек излучает слишком мало тепла.)

- Закрытость существующих разработок. Все известные нам решения являются закрытыми и проприетарными. Но проблема слишком сложна, чтобы решать её небольшой кучкой людей, а не пытаться привлечь всех готовых помочь. Поэтому мы и разрабатываем полностью Open Source-решение: странно думать, что тема, привлекающая такое количество работающих "в полях" добровольцев, не будет настолько же интересна IT-специалистам.

- Отсутствие свободы распространения. Добровольческие ПСО зачастую не централизованы, работающие подходы и приложения передаются из рук в руки, ПО с лицензируемыми копиями тут не подойдёт. Именно по этому мы в том числе выбрали стратегию открытого кода и открытого распространения, чтобы любой человек мог скачать наше решение и использовать его. (Правда, потом уже не разберёшь, где наши наработки, а где "нейроночка билайна").

- Популярные архитектуры нейросетей, используемые в известных нам решениях - YOLO, SSD, VGG - имеют хорошие метрики качества на публичных датасетах вроде ImageNet, но плохо работают на снимках в нашей, довольно специфичной, доменной области. (О выборе архитектуры нейросети, испробованных вариантах и особенностях используемой в итоге - ниже.)

- Практически никто не использует возможности по оптимизации моделей для инференса. В районах поиска часто нет выхода в интернет, поэтому нужно обрабатывать полученные снимки локально. Большинство спасателей пользуются ноутбуками с маломощными GPU, или вовсе без них, запуская нейросети на обычных CPU. Легко посчитать, что если на обработку одного снимка тратится в среднем 10 секунд, 1000 снимков будут обрабатываться около 3 часов. Тут, можно сказать, важна каждая секунда.

В то же время, используемый нами "bottom-up" подход, отталкивающийся от запросов и средств озвученных ПСО, начал приносить первые плоды, и мы планируем и дальше увеличивать сложность решения в итерационном стиле.

### Подготовка данных  

Как всем известно, театр начинается с вешалки, река с ручейка, дружба с улыбки, а проект по машинному обучению - с DataSet-а. проКазалось бы, если каждая поисковая операция с использованием БПЛА приносит тысячи фотографий, массив накопленных данных должен быть огромен - бери и обучай. Не всё оказалось так просто, потому что:

- Нет централизованного хранилища для размеченных данных. Снимки, полученные в ходе поисковых операций, в дальнейшем никак не используются и не обрабатываются.

- Полученные данные при этом очень несбалансированные. На один снимок с найденным человеком приходится несколько тысяч "пустых" фотографий. Поскольку информация о просмотренных снимках нигде не фиксируется, чтобы найти среди них нужные, надо второй раз проделать огромный объём работы - уже силами маленькой команды,  не имеющей "намётанных глаз".

- Каждое изображение, само по себе тоже "несбалансированное". Посмотрите на фотографию выше: искомый человечек занимает на ней мизерную часть всей площади снимка. Очевидно, что хорошая нейросетка должна уметь не только сказать, что на снимке, по её мнению, присутствует человек - она должна обвести конкретное место (т.е. выполнить задачу детектирования объектов, а не классификации изображений). Иначе оператор будет тратить лишнее время и силы на разглядывание, а так же может по ошибке отбраковать нужное фото. Но для этого и учиться нейросеть должна на размеченных данных, на фотографиях, где искомый объект отмечен с использованием специального ПО. Никто не будет заниматься этим во время поисковой операции - не до того. 

- Не учитывается статистика по позам, в которых находились найденные люди, время года, тип местности и другие особенности снимков. Такие данные очень бы пригодились для создания "синтетических" обучающих изображений при помощи постановочной съёмки, фоторедакторов или генеративных моделей - но для использования всего этого нужно понимать, как выглядит фотография с реально потерявшимся человеком. Сейчас при воссоздании таких фотографий приходится опираться на субъективный опыт экспертов-спасателей. 

- Помимо технических сложностей, возможны юридические препятствиями, накладывающими ограничения на право собственности полученных снимков. Зачастую, наши обращения за помощью в сборе данных вовсе остаются без ответов. Из-за отсутсвия таких данных, юридических проблем или банальной лени - неясно.

Таким образом, ценная информация никак не используется для обучения нейросетей, теряясь или оседая мёртвым грузом где-то на дисках и облачных хранилищах, вместо того, чтобы улучшать объём и качество обучающей выборки. Мы пишем сервис, который позволит, в том числе, загружать к нам ценные фото (о нём тоже ниже), но задач, как всегда больше, чем людей.

При этом на сегодняшний день в сети очень мало хороших (открытых) датасетов со снимками с БПЛА. Наиболее подходящий из найденных нами - это [Stanford Drone Dataset (SDD)](http://cvgl.stanford.edu/projects/uav_data/). Представляет собой снимки с высоты над кампусом университета, с отмеченными объектами класса "Pedestrian" (пешеход), совместно с велосипедистами, скейтбордистами и машинами. 
Несмотря на схожий угол съёмки, сфотографированные пешеходы и окружение имеют мало общего с происходящим на наших снимках. Проведенные на данном датасете эксперименты показали, что метрики качества обученных на нём детекторов на наших данных показывают низкий результат. В итоге, мы сейчас используем SDD для обучения так называемого backbone, извлекающего верхнеуровневые фичи, а верхние слои приходится доучивать на изображениях нашей доменной области.

Именно поэтому мы сначала долго общались с различными поисковиками и спасателями, пытаясь понять, как выглядит потерявшийся в лесу человек на снимке с высоты. В результате мы собрали уникальную статистику по 24 позам, в которых чаще всего находят без вести пропавших людей. Отсняли и разметили первую версию Lacmus Drone Dataset (LaDD), включающую в себя более 400 снимков. Съемка велась преимущественно с помощью DJI Mavic Pro и Phantom с высоты 50 - 100 метров, разрешение снимков 3000х4000, размер человека в среднем 50х100 px. На данный момент у нас уже четвертая версия дата сета с 5 тысячами снимков, как реальных, так и "смоделированных". 

По мере пополнения нашего датасета, мы пришли к необходимости разделять снимки по временам года. Дело в том, что модель, тренированная на зимних фото показывает результаты лучше, чем тренированная на всем датасете или на лете, или весне отдельно. Возможно, признаки на снежном фоне извлекаются лучше, чем на зашумленной траве. В то же время, при обучении только на зимних снимках количество ложных срабатываний (false positive) возрастает. Видимо, снимки разных сезонов представляют собой слишком разные ландшафты (домены) и нейросеть оказывается не в состоянии их обобщить. С этим ещё предстоит разобраться, и пока нам видится два способа:

- Сделать много "маленьких" сеточек и учить их для разных доменов отдельно (для зимы одна, для лета другя... можно кроме времен года бить еще и по местности: например отдельная модель для средней полосы и равнин, для юга другая и так далее).

- Многократно увеличить наши данные и пытаться обучить модель сразу на все домены. Основываясь на [этой](https://habr.com/ru/company/yandex/blog/431108/) статье от Яндекса мы решили попробовать данный вариант. Поэтому, возможно, у нас скоро появятся GAN-ы.

![](img/winter.png)  
  

### Процесс обучения  

Характер наших снимков значительно отличается от изображений популярных датасетов вроде ImageNet, COCO и т.д. Из-за этого разрабатываемые и тестируемые на данных наборах архитектуры нейросетей могли плохо подходить под наши задачи, и было необходимо провести исследование их применимости. Несколько популярных архитектур было протестировано на первой версии датасета, т.е. на зимних снимках. Для бэкбоунов мы брали модели, предобученные на ImageNet, отрывали последний слой и дообучали на Stanford Drone Dataset. Детекторы обучались непосредственно на наших снимках. Лучшие метрики представлены в таблице:  

Тип | mAP | FPS (GPU / CPU)
--- | --- | :---:
SSD | 0.56 |  15 / 150
YOLOv3 | 0.72 |  20 / 194
RetinaNet/mobileNetv2 | 0.67 |  32 / 300+
RetinaNet/ResNet50 | 0.91 |  1 / 45
DarkNet |   0.89 | 10 / 80
Unet | 0.96 | 0.5 / 20+

Помимо цифр в вышеприведённой таблице, следует обратить внимание на такую особенность снимков Lacmus Drone Dataset, как большой дисбаланс классов: отношение площади фона к площади прямоугольного анкора с искомымым объектом составляет несколько тысяч.

При обучении детектора это влечет за собой две проблемы:  

- Большинство регионов с фоном не несут никакой полезной информации.  
- Но регионы с объектами ввиду их малой численности так же не вносят существенного вклада в обучение весов.

Одна из протестированнах нами архитектур нейросетей, RetinaNet направлена как раз на снижение негативных последствий большого дисбаланса классов. Создатели RetinaNet проектировали её для увеличения точности one-stage detectors (покрывающих снимок густой сетью предопределённых прямоугольников-анкоров и потом выбирающих те, что лучше всего охватывают объект) по сравнению с более качественными, но медленными two-stage detectors (обучающимися вначале находить регионы-кандидаты, потом уточнятьих положение). С точки зрения авторов статьи про RetinaNet, one-stage detectors проигрывают в точности именно из-за дисбаланса, порождаемого большим количеством пустых анкоров. На фоне данного преимущества  наш выбор был сделан в пользу именно RetinaNet с бэкбоуном ResNet50.     

Архитектура этой сети [была представлена](https://arxiv.org/abs/1708.02002) в 2017 году. Главная особенность RetinaNet, позволяющая бороться с негативным влиянием дисбаланса классов при обучении Classification Subnet - это оригинальная функция потерь **Focal Loss**.

$$ FL(p_t) = − (1 − p_t)^γ log(p_t) $$    

$$
p_t =
\begin{cases}
  p, if y = 1,
  1 − p otherwise
\end{cases}
$$

Где p - это оценённая моделью вероятность объекта искомого класса (попросту говоря, выхода нейросети, если он приводится к промежутку [0,1]).
 
В других доменных областях loss-функция, как правило, должна быть робастной к нетипичным экземплярам (hard examples), являющимся скорее всего выбросами; их влияние влияние на обучение весов должно быть сокращено. В Focal Loss наоборот, снижается влияние часто встречающегося фона (inliers, easy examples), а наибольшее влияние при обучении весов RetinaNet оказывают редко встречающиеся объекты. Делается это за счёт вот этой части формулы:

$$(1 − p_t)^γ$$

Коэффициент γ в показателе степени определяет вклад hard examples в суммарную функцию потерь. 

Структурно RetinaNet состоит из бэкбоуна и двух дополнительных сетей классификации (Classification Subnet) и определения границ объекта (Box Regression Subnet).

![](img/retina.png)


В качестве бэкбоуна используется сверточная нейросеть, имеющая дополнительные выходы со скрытых слоев (**Feature Pyramid Network, FPN**).
Она позволяет выделить из исходного изображения пиримиду признаков в разных масштабах, на которых могут быть обнаружены как большие, так и мелкие объекты.
FPN используется во многих архитектурах, улучшая детекцию объектов разного масштаба: RPN, DeepMask, Fast R-CNN, Mask R-CNN и т.д.  
Более подробно про FPN можно почитать в [оригинальной статье](FPN).

В нашей сети, как и в оригинальной, использована FPN из ResNet50, с дополнительными выходами со слоев с 3 по 7,
Все уровни пирамиды имеют одинаковое количество каналов С = 256 и количество анкоров А около 1000 (зависит от размера изображений).  

Анкоры (границы объектов) имеют площади от [32 х 32] до [512 x 512] с шагом смещения (strides) [16 - 256] px на каждом уровне пирамиды признаков. Такой размер мы подобрали потому что зачастую приходится анализировать мелкие обьекты и некоторую окрестность вокруг. Например пветка, если не учитывать окружающую ее действительность очень похожа на лежащего человека.  
В оригинальной [FPN](https://arxiv.org/pdf/1612.03144.pdf) используются три значения ориентации анкоров (1:2, 1:1, 2:1).
В нашей RetinaNet для более плотного покрытия добавлено масштабирование анкоров [2^0, 2^1/3, 2^2/3].
Применение 9 анкоров на каждом уровне позволяет находить объекты c длиной / шириной от 32 до 813 px.

**Classification Subnet** предсказывает вероятность присутствия для каждого из К классов в заданном анкоре.
По сути это простая полносвязная сеть (Fully ConvNet, FCN), присоединенная к каждому из уровней FPN.
Ее параметры на различных уровнях пирамиды одинаковы, и архитектура ее довольно простая:  
- на вход подается карта признаков (W x H x C),
- 3х3 свертка с С фильтрами,
- ReLU активация,
- 3х3 свертка с (К х А) фильтрами,
- сигмоид активация последнего слоя.  

Итого на выходе этой сети формируется вектор длиной К, по количеству классов для разных объектов.
В нашем случае используется только один класс - это Pedestrian.


**Box Regression Subnet** позволяет более точно подогнать 4-вектор координат анкора под размер объекта.
Это небольшая полносвязная сеть, прикрепленная к каждому из уровней FPN, которая работает независимо от Classification Subnet.
Их архитектуры почти одинаковы, за исключением того, что при обучении минимизируется вектор размером (4 х А) - $ (\Delta x_min, \Delta y_min, \Delta x_max, \Delta y_max) $ для каждого анкора.  

Считается, что анкор содержит объект, если IoU (Intersection over Union) > 0.5.
В этом случае y_i назначается 1, иначе 0.
Такой подход позволяет сократить вычислительные затараты при обучении деетктора.

В процессе обучения RetinaNet функция потерь вычисляется для всех ориентаций анкоров, на каждом слое FPN (всего около 10 000 областей для одного изображения).  
Это в корне отличается от известных подходов эвристической выборки (RPN) или поиска редких экземпляров (OHEM, SSD) с выбором небольшого количества областей (около 256) для каждого минибатча.  

Значение Focal loss вычисляется как сумма значений функции для всех областей, нормализованных по отношению к анкорам, классифицированных как "фон".
Нормализация производится только по анкорам, содержащим объект, а не общему их числу.
Этот подход как раз и позволяет избежать нежелательного влияния большого числа отрицательных классов.


**Инференс** (детектирование объектов на изображении) заключается в вычислении forward функции бэкбоуна и двух подсетей.
Для повышения скорости классификация осуществляется только по тем областям,  у которых y > 0,05.
При этом в оригинальном детекторе дополнительно накладывается ограничение на максимальное количество областей на изображении – не более 1000.
На последней стадии детекции остаются только те области, вероятность классификации для которых превышает threshold = 0,5.

Более подробно про архитектуру RetinaNet можно еще почитать [здесь](https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4).

Для нас очень важна скорость работы сети. По этому мы долго и упорно искали решения для того чтобы выжать максимум из среднего ноутбука за 20-30 тыс р. Вот лишь несколько подходов и автоматических фреймворков, которые мы попробывали:

- Intel OpenVINO - для оптимизации на CPU (в итоге мы скомпилировали нашу сеть, но по каким то причинам эффекта это не дало). 
- TensorRT - для оптимизации на GPU (наша сеть не конвертировалась). 
- PlaidML - запускали нашу сеточку на Intel HD Graphics (!) (сеточка заработала но выйгрыш в скорости по сравнению c CPU получился очень маленьким). 
- nGraph - прироста в производительности не получили.

А еще...

- nvidia jetson, 
- Coral Edge TPU.

В результате мы собрали свою версию tensoflow 1.14 с оптимизациями для CPU с поддержкой AVX используя различные хитрости и библиотеки от Intel вроде nndl. 
Не обошли стороной мы и старые процессоры без AVX инструкций (в основном выпускались до 2012 года) и теперь мы можем сказать, что наше решение работает даже на *Core 2 Duo*! 
Также мы добавили [некоторые оптимизации](https://github.com/lacmus-foundation/lacmus/wiki/%D0%94%D0%BB%D1%8F-%D1%80%D0%B0%D0%B7%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%87%D0%B8%D0%BA%D0%BE%D0%B2:-%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F-OpenCV-LUT), связанные со скоростью загрузки и аугментации данных c использованием OpenCV LUT и больших uint8-матриц. 
Это позволило ускорить загрузку данных в сеть в несколько раз, что значительно сократило время обучения и инференса модели.

Сухая статистика:

| **Оборудование**                                             | **Время работы для картинка 3000x4000 px, c** |
| ------------------------------------------------------------ | --------------------------------------------- |
| i5-3337U (2x cores / 4x threads @ 1,80 Hz) AVX               | 2.3                                           |
| i5-6200U (2x cores / 4x threads @ 2,30 Hz) AVX2              | 1.7                                           |
| i7-8750H (8x cores / 12x threads @ 2,20 Hz) AVX2             | 0.8                                           |
| i7-9700 (8x cores / 8x threads @ 3.6Нz) AVX2                 | 0.5                                           |
| NVIDIA GeForce GTX 950M (2gb vRAM)                           | 0.3                                           |
| NVIDIA GeForce GTX 2080Ti (11gb vRAM)                        | 0.02                                          |
| **Core 2 Duo E8400 (2x cores / 2x threads @ 3,0 Hz) no avx** | **7**                                         |



### Production

**Модель нейронной сети и docker**

С учетом пожеланий и запросов спасателей из Liza Alert, нами разработано desktop приложение. В действительности ты хотели максимально упростить работу пользователя и облегчить установку необходимых библиотек. Поставить nvidia Cuda и CuDNN или собрать кастомую сборку tensorflow не самая простая задача для пользователя. К тому же мы хотели чтобы пользователю не пришлось заботится установкой python и настройкой зависимостей. Мы хотели что-то компактное, то что сокроет от пользователя всю сложность настройки и установки библиотек. И мы нашли это решение - это Docker. В контейнере мы разворачиваем лакальный маленький web-сервер с нейронкой на борту и установленными зависимостями. Если понадобится обновить модель, то нам всего лишь нужно будет скачать новую версию docker образа. В добавок, мы отделяем таким образом GUI и модель. 
В случае если у пользователя будет хороший канал связи, через GUI можно обращаться не к локальному серверу, а к удаленному.  Docker предоставляет удобное API, которым можно автоматически управлять прямо из GUI, поэтому для пользователя все останется таким же прозрачным. В добавок Docker предоставляет свой репозиторий, где можно хранить разные версии образов и не заботиться о сервере с моделями.

**Интерфейс, или С# творит чудеса**

Теперь об интерфейсе. В процессе разработки приложения нам были выдвинуты 3 критерия:

- Быстрота разработки
- Кроссплатформенность
- Низкое потребление ресурсов

После некоторых поисков в интернете мы наткнулись на статью. Хм! Это что то новенькое! Какой то очень молодой фреймворк? Интересно его потрогать, решили мы, и не прогадали! GUI написан на С# фреймворке AvaloniaUI, что позволяет запускать ее x64 OS win10, linux и mac.

AvaloniaUI Это молодой но очень мощный и быстрый фреймверк. По своей концепции он очень похож на WPF и позволяет переносить на него приложения не особо изменяя код.
Он быстр и эффективен, 2d графика в нем рисуется быстрее и потребляет меньше ресурсов, чем у WPF. Также тут есть некоторые плюшки, улучшающие оригинальный WPF.

Что касается внутреннего устройства то тут применяется библиотека SkiaSharp для отрисовки графики и GTK (для Unix систем). Также ведётся разработка X11 рендера.
Всё это позволяет рисовать интерфейс везде где угодно, даже в буфере консоли(!).
Если бы dotnet core можно было бы запустить в Bios, то AvaloniaUI отрисовала бы там модный геймерский интерфейс, как на крутых материнских платах.

AvaloniaUI набирает популярность и является открытым фреймворком, хотя все еще находится в бете и в этом фремверке присутствуют баги. Но разработчики постоянно улучшают и дополняют фреймверк. И по состоянию на конец 2019 года мы можем заявить что на нем уже можно писать не большие коммерческие решения. Если вы знакомы с WPF и C# - то вам определенно стоит попробовать. К достоинствам можно также отнести низкое потребление ресурсов интерфейсом (что не скажешь об electron), а значит у мы выигрываем несколько мегабайт ОЗУ для нашей сеточки.

![](img/avaloniaUI.gif)

*вот на что он способен...*

Также хочется сказать что у проекта довольно отзывчивая поддержка и разработчики быстро отвечают на issue. Те кому интересно - могут почитать [нашу статью](https://gosha20777.github.io/dotnet/2019/05/24/avalonui/), еще [эту](https://habr.com/ru/post/447152/) статью. И [эту](https://habr.com/ru/post/438920/). 
Для полного понимания этой концепции стоит посмотреть [выступление](https://youtu.be/8qzqweimcFs) Никиты Цуканова [@kekekeks](https://github.com/kekekeks).
Он является разработчиком этого фреймверка, отлично разбирается в нем и в dotnet в общем.

**Бекенд**

Еще помимо desktop приложения нами разработана mlOps инфраструктура для проведения экспериментов по поиску лучшей архитектуры нейросети в облаке.  С помощью серверной части мы хотим:

- аггрегировать данные и централизовано их хранить,  
- автоматизировать процесс обучения нейронной сети и предоставлять окружения для обучения для других людей чтобы везти исследования,  
- предоставлять облако посиково-спасательным отрядам чтобы они могли обработать большой объем данных, если такие случаются.

![](img/backend.jpg)  

**Desktop-client** может работать как с локальной версией docker-контейнера, так и последней версией на центральном сервере, через REST API.  
**Identity** микросервис обеспечивает доступ к серверу только авторизованных пользователей.  
**Dataset** сервис служит для хранения как самих изображений, так и их разметки.  
**Predict** сервис позволяет осуществлять быструю обработку большого количества изображений при наличии широкого канала у пилотов.  
**Training** сервис нужен для тестирования новых моделей и дообучения существующих по мере поступления новых данных.  
Управление очередью задач осуществляется с помощью RabbitMQ / Redis.  

GPU-сервер предоставил нам один из участников проекта, однако мы с удовольствием будем сотрудничать и с новыми партнерами.  

### Итого  

За прошедший 2019 год участники Lacmus Foundation:  

- отсняли и разметили уникальный dataset, последняя версия которого включает в себя более 4000 снимков.

- попробовали ряд различных DL подходов и выбрали лучший,

- подобрали лучшие гиперпараметры нейронной сети и обучили ее на собственных уникальных данных для лучшего распознавания;

- разработали кросс-платформенное приложение для операторов БПЛА с возможностью использования нейронной сети на местах ПСР при работе оффлайн;

- оптимизировали работу нашей нейронной сети для работы на бюджетных и маломощных портативных компьютерах;

- создали средства защиты нашей программы от неправомерного использования.

Наша программа готова к использованию в условиях реальных ПСР и прошла тестирование на генеральных прогонах.
На открытых участках типа «поле» и «бурелом» обнаружены все тестовые «потерявшиеся».
На данный момент лучшие показатели метрики mAP нашей нейронной сети "Лакмус" – 94%.
Результаты по найденным объектам нейронной сетью "Лакмус" заинтересовали добровольческие поисково-спасательные отряды со всей России.
На счету нашей программы уже не одна спасенная жизнь.  

 В следующем году мы планируем:  

- найти партнера для надежного хостинга инфраструктуры,
- реализовать веб интерфейс и mlOps,
- сформировать большой синтетический датасет на движке UE4 или с помощью GANов
- запустить In Class соревнование на Kaggle для всех желающих прокачать свои DL/CV навыки и поиска наилучших SOTA решений.  

Нам очень не хватает рабочих рук для того чтобы реализовать эти планы, поэтому мы будем рады всем, независимо от уровня и направления подготовки.

Ведь если вместе мы сможем спасти хотя бы еще одного человека, то все приложенные усилия будут не зря.

### Казалось бы причем тут Билайн?

Скажем честно мы до последнего не хотели поднимать шумиху и пытались решить все ТэТаэт. Однако Компания Билайн проигнорировала наши притенезии и продолжила делать странные вещи. Некоторое время назад компания Билайн [развернула](https://moskva.beeline.ru/about/press-center-new/press-releases/details/1482769/) масштабную рекламную компанию в которой говорила о том что разработала "уникальное" решение. Подобные новости легко ищутся по запросу "Билайн AI ищет людей". К нам в руки попала "их" программы и мы можем заявить о том что **наш код украли**. Притом украли не только код но и модель обученную нейронной сети и наш датасет. Мы open sourue проект и все наши продукты  распространяются под лицензией GNU. Мы не запрещаем копировать и использовать наш код: все обученные модели и все данные открыты. Более того мы будем даже рады если такая компания как билайн будет использовать наши наработки. Но лицензия GNU **запрещает** создание на ее основе *проприетарных* программ и требует *соблюдения авторского права*, но к сожалению во всех своих многочисленных статьях компания билайн не прикрепляла ссылки на наш проект и выдавала наше решение за свое. В связи с бездействием компании билайн а также в свете последних событий мы вынуждены заявить об этом гласно.

Что самое забавное то что операторы БПЛА в самой Лиза Алерт используют наше ПО и мы по мере возможности помогаем им и оказываем поддержку, однако Билайн это не смущает и они "ищут" людей на данных которые мы собрали и разметили год назад:

![](img/bad_beline1.png) 

*Съемка произведена в феврале 2019 в яхт-глубе Галс, город москва. Немного дадим комментариев к фотографии, чтобы был понятен объем работ. В съемке участвовали такие люди как: Георгий Перевозчиков, Екатерина Быкова, Виктория Мартынова, Дмитрий Ружицкий Михаил Шуранков и другие. В конкретно этой съемке принимало участие 20 человек (включая операторов, и тех людей которые отбирали фотографии). Для достоверности набора данных в качестве статистов использовались не только взрослые, но и дети. А провезти ребенку целый день в лесу - это подвиг. Съемка обычно занимает весь день с утра и до вечера. За один заход удается получить после ручного отбора и разметки 300-500 разных фотографий которые потом включаются в датасет. На разметку этого залета у нас ушло двое суток - все выходные(!). Просматривать тысячи снимков и выбирать из них наиболее лучшие, а потом руками обводить каждого человека довольно утомительная задача. В результате на итоговые 500 снимков с людьми приходится порядка 150 человека часов... Данные - одно из самых трудоемких процессов в работе ML инженера. Наш датасет можно получить обратившись к нам лубой желающий, данные также распостраняются по лицензии GNU, однако до этого он был доступен всем желающим по ссылке (убрали из публичного доступа по понятным причинам). В датасете был прикреплен файл c лицензией и со списком имен тех, кто участвовал в его создании, говорящий о том что любой кто использует наши данные должен давать ссылку на наш проект и на источник. На этом сайте присутствуют также другие снимки из нашего набора данных. Но к сожалению команде Lacmus не удалось найти на этом ресурсе ссылок на наш проект.*

![](img/bad_beline2.png)

*no comments*

Ниже прикреплен подробный отчет и подробное сравнение исходных кодов и моделей нашего проекта и проекта компании билайн. Мы требуем соблюдение лицензии GNU и раскрытия компанией билайн всех исходных кодов прямо или косвенно использующих наши наработки и датасеты. Мы будем рады если команда Билайн AI присоединится к нашему проекту - в конце конков мы делаем общее дело, но мы не хотим чтобы против нас вели войну. Команда lacmus возмущена действиями некоторых российских компаний в отношении open sourse проектов. Open sourse технологии должны быть свободными.

- [Сравнительный анализ проекта Lacmus и проекта Билайн.AI](https://docs.google.com/document/d/1xyk14ksnzGcQz1vSioON0pAwsekrIPUJiHNmbiMKmDs/edit?usp=drivesdk) - ссылка на google docs

Вы можете цитировать и распространять этот абзац или писать на основе него другие статьи в интернете. Мы будем рады вашей поддержке.

### Как помочь проекту

Мы open sourse проект и активно примем всех желающих!

Вот ссылки на наши репозитори на github:

- https://github.com/lacmus-foundation - наше сообщество
- https://github.com/lacmus-foundation/lacmus - наша нейронная сеть
- https://github.com/lacmus-foundation/lacmus-app - наше GUI приложение

**Если вы разработчик** и хотите присоединиться к проекту - вы можете написать Перевозчиков Георгий Павловичу, @gosha20777 во всех соц сетях, gosha20777@live.ru или присоедениться к проекту прямо из ODS канала в slack (если вы там есть). Мы любим активных людей! Нам нужны

- ML разработчики
- C# / go / python разработчики
- Фронтенщдики
- Бекендщики
- Просто активные люди любых направлений! Мы всегда будем вам рады!

**Если вы не связаны с разработкой** вы тоже можете помочь проекту

- Вы можете помочь нам с написанием статей
- Вы можете помочь нам с написанием пользовательской документации и wiki (и по исправлять там грамматические ошибки)))
- Вы можете побыть в роли product менеджера и по заполнять задачки в трелло
- Вы можете предложить нам идею
- Вы можете распространить этот пост

### Вместо заключения

Данная статья писалась разными людьми из нашей команды, а потому манера изложения может отличаться от пункта к пункту. Мы все разные но обедняет нас одно - нам всем интересно пробовать что то новое, экспериментировать и создавать что то. Мы все хотим чтобы мир it технологий был открытым и полезным для многих людей, чтобы он помогал человечеству.

### Благодарности

- Самым активным участникам ODS в канале #proj_rescuer_la : @Kseniia @balezz @ei-grad @dartov @sharov_am @Palladdiumm.
- Участникам проекта вне ODS: Мартынова Виктория Викторовна (организация проекта, сбор и разметка данных), Шуранков Денис Петрович (организация сбора данных), Перевозчикова Дарья Павловна (разместила около 30% всех фото).
- Программистам из avaloniaui - лучшего фреймверка на dotnet: @worldbeater @kekekeks @lary9896
- Операторам БПЛА из отряда Лиза Алерт, за снимку и набор данных: Партызан, Вантеич, Севыч, Калифорния, Тарекон, Евген, ГБ.
- Админам ODS за организацию самого крутого сообщества: @natekin, @Sasha, @mephistopheies.  

Руководитель проекта, Перевозчиков Георгий Павлович, @gosha20777.

Этот список не полный, в действительности он гораздо больше.
